{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343c9ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import unicodedata\n",
    "import re\n",
    "from env import user, password, host, db, protocol\n",
    "import acquire\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "from nltk import ngrams\n",
    "\n",
    "from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca5f10a",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "Do your work for this exercise in a file named explore.\n",
    "\n",
    "1. Spam Data\n",
    "* Load the spam data set.\n",
    "* Create and explore bigrams for the spam data. Visualize them with a word cloud. How do they compare with the ham bigrams?\n",
    "    * spam seems to be urging a reply for some offer or advertisement. ham words seem to be informative or basic daily conversation\n",
    "\n",
    "* Is there any overlap in the bigrams for the spam data and the ham data?\n",
    "    * 'call' is a frequent word in both spam and ham\n",
    "\n",
    "* Create and explore with trigrams (i.e. a n-gram with an n of 3) for both the spam and ham data.\n",
    "\n",
    "2. Explore the blog articles using the techniques discussed in the exploration lesson.\n",
    "\n",
    "3. Explore the news articles using the techniques discussed in the exploration lesson. Use the category variable when exploring."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7f9b65",
   "metadata": {},
   "source": [
    "# Data Exploration\n",
    "\n",
    "* In this lesson, we'll be taking a look at a data set that contains SMS messages that are labelled as either a spam text message, or an actual text message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549a93fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_db_url(database, host=host, user=user, password=password):\n",
    "    return f'mysql+pymysql://{user}:{password}@{host}/{database}'\n",
    "\n",
    "\n",
    "url = get_db_url(\"spam_db\")\n",
    "sql = \"SELECT * FROM spam\"\n",
    "\n",
    "df = pd.read_sql(sql, url, index_col=\"id\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1da46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'spamham_data'\n",
    "df.to_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3165d935",
   "metadata": {},
   "outputs": [],
   "source": [
    "ADDITIONAL_STOPWORDS = ['r', 'u', '2', 'ltgt']\n",
    "\n",
    "def clean(text):\n",
    "    'A simple function to cleanup text data'\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    stopwords = nltk.corpus.stopwords.words('english') + ADDITIONAL_STOPWORDS\n",
    "    text = (unicodedata.normalize('NFKD', text)\n",
    "             .encode('ascii', 'ignore')\n",
    "             .decode('utf-8', 'ignore')\n",
    "             .lower())\n",
    "    words = re.sub(r'[^\\w\\s]', '', text).split()\n",
    "    return [wnl.lemmatize(word) for word in words if word not in stopwords]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9121f6a6",
   "metadata": {},
   "source": [
    "* Let's first take a look at how many of the messages are spam vs ham:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a910e763",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.concat([df.label.value_counts(),\n",
    "                    df.label.value_counts(normalize=True)], axis=1)\n",
    "labels.columns = ['n', 'percent']\n",
    "labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0e1016",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_counts_and_ratios(df, column):\n",
    "    \"\"\"\n",
    "    Takes in a dataframe and a string of a single column\n",
    "    Returns a dataframe with absolute value counts and percentage value counts\n",
    "    \"\"\"\n",
    "    labels = pd.concat([df[column].value_counts(),\n",
    "                    df[column].value_counts(normalize=True)], axis=1)\n",
    "    labels.columns = ['n', 'percent']\n",
    "    labels\n",
    "    return labels\n",
    "\n",
    "show_counts_and_ratios(df, \"label\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd99dc0",
   "metadata": {},
   "source": [
    "Let's break the data up into 3 seperate pieces:\n",
    "\n",
    "* The words that appear in legitimate text messages.\n",
    "\n",
    "* The words that appear in spam text messages.\n",
    "\n",
    "* All of the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f114b033",
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_words = clean(' '.join(df[df.label == 'ham'].text))\n",
    "spam_words = clean(' '.join(df[df.label == 'spam'].text))\n",
    "all_words = clean(' '.join(df.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a75dbcd",
   "metadata": {},
   "source": [
    "Once we have a list of words, we can transform it into a pandas Series, which we can then use to show us how often each of the words occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d62c427",
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_freq = pd.Series(ham_words).value_counts()\n",
    "spam_freq = pd.Series(spam_words).value_counts()\n",
    "all_freq = pd.Series(all_words).value_counts()\n",
    "\n",
    "spam_freq.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2aba87",
   "metadata": {},
   "source": [
    "Now we'll combine these three together to get one resulting data frame that we can work with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbcfb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = (pd.concat([all_freq, ham_freq, spam_freq], axis=1, sort=True)\n",
    "                .set_axis(['all', 'ham', 'spam'], axis=1, inplace=False)\n",
    "                .fillna(0)\n",
    "                .apply(lambda s: s.astype(int)))\n",
    "\n",
    "word_counts.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92956c68",
   "metadata": {},
   "source": [
    "We can now use this data set to answer some interesting questions:\n",
    "\n",
    "* What are the most frequently occuring words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b05727",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts.sort_values(by='all', ascending=False).head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b727d4e1",
   "metadata": {},
   "source": [
    "Are there any words that uniquely identify a spam or ham message?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239bbf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([word_counts[word_counts.spam == 0].sort_values(by='ham').tail(6),\n",
    "           word_counts[word_counts.ham == 0].sort_values(by='spam').tail(6)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fed22b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# figure out the percentage of spam vs ham\n",
    "(word_counts\n",
    " .assign(p_spam=word_counts.spam / word_counts['all'],\n",
    "         p_ham=word_counts.ham / word_counts['all'])\n",
    " .sort_values(by='all')\n",
    " [['p_spam', 'p_ham']]\n",
    " .tail(20)\n",
    " .sort_values('p_ham')\n",
    " .plot.barh(stacked=True))\n",
    "\n",
    "plt.title('Proportion of Spam vs Ham for the 20 most common words')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb20f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "(word_counts\n",
    " [(word_counts.spam > 10) & (word_counts.ham > 10)]\n",
    " .assign(ratio=lambda df: df.spam / (df.ham + .01))\n",
    " .sort_values(by='ratio')\n",
    " .pipe(lambda df: pd.concat([df.head(), df.tail()])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9c1743",
   "metadata": {},
   "source": [
    "# Word Clouds\n",
    "\n",
    "The wordcloud allows you to identify the relative frequency of different keywords using an easily digestible visual.\n",
    "\n",
    "# Common Use Cases\n",
    "\n",
    "As a visualization technique, this method gives a more qualitative analysis of the topics in the documents.\n",
    "\n",
    "\n",
    "**Pros**\n",
    "\n",
    "* It’s intuitive and easy to comprehend.\n",
    "* It helps identify overall respondent sentiment and the specific factors that drive it.\n",
    "* It provides direction for further analysis.\n",
    "\n",
    "**Cons**\n",
    "\n",
    "* It fails to measure each word’s value in and of itself.\n",
    "* It allows irrelevant words to appear.\n",
    "* When words appear similar in size, it becomes difficult to differentiate them.\n",
    "\n",
    "First we'll take a look at a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bd1d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'Mary had a little lamb, little lamb, little lamb. Its fleece was white as snow.'\n",
    "\n",
    "img = WordCloud(background_color='white').generate(sentence)\n",
    "# WordCloud() produces an image object, which can be displayed with plt.imshow\n",
    "plt.imshow(img)\n",
    "# axis aren't very useful for a word cloud\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f95d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cloud = WordCloud(background_color='white', height=1000, width=400).generate(' '.join(all_words))\n",
    "ham_cloud = WordCloud(background_color='white', height=600, width=800).generate(' '.join(ham_words))\n",
    "spam_cloud = WordCloud(background_color='white', height=600, width=800).generate(' '.join(spam_words))\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "axs = [plt.axes([0, 0, .5, 1]), plt.axes([.5, .5, .5, .5]), plt.axes([.5, 0, .5, .5])]\n",
    "\n",
    "axs[0].imshow(all_cloud)\n",
    "axs[1].imshow(ham_cloud)\n",
    "axs[2].imshow(spam_cloud)\n",
    "\n",
    "axs[0].set_title('All Words')\n",
    "axs[1].set_title('Ham')\n",
    "axs[2].set_title('Spam')\n",
    "\n",
    "for ax in axs: ax.axis('off')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a7e407",
   "metadata": {},
   "source": [
    "# Bigrams\n",
    "\n",
    "Bigrams are a specific instance of the broader concept of n-grams, which is a way to combine words together. This lets us measure not just the individual word frequency, but also takes into account which words appear together.\n",
    "\n",
    "To produce the bigrams, we'll use nltk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585db6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'Mary had a little lamb'\n",
    "\n",
    "bigrams = nltk.ngrams(sentence.split(), 2)\n",
    "list(bigrams)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22eaa85d",
   "metadata": {},
   "source": [
    "We can apply the same transformation to our ham data set in order to find out which bigrams are the most frequently occuring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862b1d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_20_ham_bigrams = (pd.Series(nltk.ngrams(ham_words, 2))\n",
    "                      .value_counts()\n",
    "                      .head(20))\n",
    "\n",
    "top_20_ham_bigrams.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3401ae3f",
   "metadata": {},
   "source": [
    "# Create and explore bigrams for the spam data. Visualize them with a word cloud. How do they compare with the ham bigrams?\n",
    "* spam seems to be urging a reply for some offer or advertisement. ham words seem to be informative or basic daily conversation\n",
    "\n",
    "# Is there any overlap in the bigrams for the spam data and the ham data?\n",
    "* 'call' is a frequent word in both spam and ham\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249c9877",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_20_spam_bigrams = (pd.Series(nltk.ngrams(spam_words, 2))\n",
    "                      .value_counts()\n",
    "                      .head(20))\n",
    "\n",
    "top_20_spam_bigrams.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b11558",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {k[0] + ' ' + k[1]: v for k, v in top_20_spam_bigrams.to_dict().items()}\n",
    "img = WordCloud(background_color='white', width=800, height=400).generate_from_frequencies(data)\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745ed862",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_20_ham_bigrams.sort_values(ascending=False).plot.barh(color='blue', width=.9, figsize=(10, 6))\n",
    "\n",
    "plt.title('20 Most frequently occuring ham bigrams')\n",
    "plt.ylabel('Bigram')\n",
    "plt.xlabel('# Occurances')\n",
    "\n",
    "# make the labels pretty\n",
    "ticks, _ = plt.yticks()\n",
    "labels = top_20_ham_bigrams.reset_index()['index'].apply(lambda t: t[0] + ' ' + t[1])\n",
    "_ = plt.yticks(ticks, labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186e2075",
   "metadata": {},
   "source": [
    "We can use these bigrams to make a word cloud as well, with a little more effort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08c4fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can supply our own values to be used to determine how big the words (or\n",
    "# phrases) should be through the `generate_from_frequencies` method. The\n",
    "# supplied values must be in the form of a dictionary where the keys are the\n",
    "# words (phrases), and the values are numbers that correspond to the sizes.\n",
    "#\n",
    "# We'll convert our series to a dictionary, and convert the tuples that make up\n",
    "# the index into a single string that holds each phrase.\n",
    "\n",
    "\n",
    "data = {k[0] + ' ' + k[1]: v for k, v in top_20_ham_bigrams.to_dict().items()}\n",
    "img = WordCloud(background_color='white', width=800, height=400).generate_from_frequencies(data)\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138d0336",
   "metadata": {},
   "source": [
    "# Create and explore with trigrams (i.e. a n-gram with an n of 3) for both the spam and ham data.\n",
    "\n",
    "# Spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebc6d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'Mary had a little lamb'\n",
    "\n",
    "trigrams = nltk.ngrams(sentence.split(), 3)\n",
    "list(trigrams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4dbc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_20_spam_trigrams = (pd.Series(nltk.ngrams(spam_words, 3))\n",
    "                      .value_counts()\n",
    "                      .head(20))\n",
    "\n",
    "top_20_spam_trigrams.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6862cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {k[0] + ' ' + k[1]: v for k, v in top_20_spam_trigrams.to_dict().items()}\n",
    "img = WordCloud(background_color='white', width=800, height=400).generate_from_frequencies(data)\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5066086",
   "metadata": {},
   "source": [
    "# Ham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926b054b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'Mary had a little lamb'\n",
    "\n",
    "trigrams = nltk.ngrams(sentence.split(), 3)\n",
    "list(trigrams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee582416",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_20_ham_trigrams = (pd.Series(nltk.ngrams(ham_words, 3))\n",
    "                      .value_counts()\n",
    "                      .head(20))\n",
    "\n",
    "top_20_ham_trigrams.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44e218b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {k[0] + ' ' + k[1]: v for k, v in top_20_ham_trigrams.to_dict().items()}\n",
    "img = WordCloud(background_color='white', width=800, height=400).generate_from_frequencies(data)\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2073d09",
   "metadata": {},
   "source": [
    "# Explore the blog articles using the techniques discussed in the exploration lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5c6848",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df = acquire.get_news_articles_data(refresh=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d522cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "codeup_df = acquire.get_blog_articles_data(refresh=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5746b969",
   "metadata": {},
   "source": [
    "# News "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a3ca4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577b2f59",
   "metadata": {},
   "source": [
    "# Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ac3437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all the article texts into a single string\n",
    "text = ' '.join(news_df['content'])\n",
    "\n",
    "# generate the word cloud\n",
    "wordcloud = WordCloud(width=800, height=800, background_color='white', max_words=100).generate(text)\n",
    "\n",
    "# plot the word cloud\n",
    "plt.figure(figsize=(8, 8), facecolor=None)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d55ddc6",
   "metadata": {},
   "source": [
    "# Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d49bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the text\n",
    "tokens = nltk.word_tokenize(text)\n",
    "\n",
    "# create a bigram finder\n",
    "bigram_measures = BigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(tokens)\n",
    "\n",
    "# apply filters to the bigrams\n",
    "finder.apply_freq_filter(3)\n",
    "finder.apply_word_filter(lambda word: word in nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "# get the top 10 bigrams by PMI\n",
    "bigrams = finder.nbest(bigram_measures.pmi, 10)\n",
    "\n",
    "# print the top 10 bigrams\n",
    "print(\"The top 10 bigrams by PMI:\")\n",
    "for bigram in bigrams:\n",
    "    print(bigram)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a98cd5",
   "metadata": {},
   "source": [
    "# Trigams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbef51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all the article texts into a single string\n",
    "text = ' '.join(news_df['content'])\n",
    "\n",
    "# tokenize the text\n",
    "tokens = nltk.word_tokenize(text)\n",
    "\n",
    "# generate trigrams\n",
    "trigrams = list(ngrams(tokens, 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdfe939",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Example trigrams:\")\n",
    "for i in range(5):\n",
    "    print(next(iter(trigrams)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24af3039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the frequency of each trigram\n",
    "freq_dist = nltk.FreqDist(trigrams)\n",
    "\n",
    "# print the most common trigrams\n",
    "print(\"The most common trigrams:\")\n",
    "for trigram, count in freq_dist.most_common(10):\n",
    "    print(f\"{trigram}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0ab297",
   "metadata": {},
   "source": [
    "# Blog "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9aa40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "codeup_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099352b2",
   "metadata": {},
   "source": [
    "# Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2078d275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all the article texts into a single string\n",
    "text = ' '.join(codeup_df['content'])\n",
    "\n",
    "# generate the word cloud\n",
    "wordcloud = WordCloud(width=800, height=800, background_color='white', max_words=100).generate(text)\n",
    "\n",
    "# plot the word cloud\n",
    "plt.figure(figsize=(8, 8), facecolor=None)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbd1acf",
   "metadata": {},
   "source": [
    "# Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd5bd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all the article texts into a single string\n",
    "text = ' '.join(codeup_df['content'])\n",
    "\n",
    "# tokenize the text\n",
    "tokens = nltk.word_tokenize(text)\n",
    "\n",
    "# create a bigram finder\n",
    "bigram_measures = BigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(tokens)\n",
    "\n",
    "# apply filters to the bigrams\n",
    "finder.apply_freq_filter(3)\n",
    "finder.apply_word_filter(lambda word: word in nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "# get the top 10 bigrams by PMI\n",
    "bigrams = finder.nbest(bigram_measures.pmi, 10)\n",
    "\n",
    "# print the top 10 bigrams\n",
    "print(\"The top 10 bigrams by PMI:\")\n",
    "for bigram in bigrams:\n",
    "    print(bigram)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771bfcf7",
   "metadata": {},
   "source": [
    "# Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32541fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all the article texts into a single string\n",
    "text = ' '.join(codeup_df['content'])\n",
    "\n",
    "# tokenize the text\n",
    "tokens = nltk.word_tokenize(text)\n",
    "\n",
    "# generate trigrams\n",
    "trigrams = list(ngrams(tokens, 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5651c01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Example trigrams:\")\n",
    "for i in range(5):\n",
    "    print(next(iter(trigrams)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47f9f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the frequency of each trigram\n",
    "freq_dist = nltk.FreqDist(trigrams)\n",
    "\n",
    "# print the most common trigrams\n",
    "print(\"The most common trigrams:\")\n",
    "for trigram, count in freq_dist.most_common(10):\n",
    "    print(f\"{trigram}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28454f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
