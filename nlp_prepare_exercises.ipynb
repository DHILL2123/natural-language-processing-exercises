{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36a1c9dc",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "* The end result of this exercise should be a file named prepare.py that defines the requested functions.\n",
    "\n",
    "* In this exercise we will be defining some functions to prepare textual data. These functions should apply equally well to both the codeup blog articles and the news articles that were previously acquired.\n",
    "\n",
    "1. Define a function named basic_clean. It should take in a string and apply some basic text cleaning to it:\n",
    "* Lowercase everything\n",
    "* Normalize unicode characters\n",
    "* Replace anything that is not a letter, number, whitespace or a single quote.\n",
    "\n",
    "2. Define a function named tokenize. It should take in a string and tokenize all the words in the string.\n",
    "\n",
    "3. Define a function named stem. It should accept some text and return the text after applying stemming to all the words.\n",
    "\n",
    "4. Define a function named lemmatize. It should accept some text and return the text after applying lemmatization to each word.\n",
    "5. Define a function named remove_stopwords. It should accept some text and return the text after removing all the stopwords.\n",
    "   This function should define two optional parameters, extra_words and exclude_words. These parameters should define any additional stop words to include, and any words that we don't want to remove.\n",
    "\n",
    "6. Use your data from the acquire to produce a dataframe of the news articles. Name the dataframe news_df.\n",
    "\n",
    "7. Make another dataframe for the Codeup blog posts. Name the dataframe codeup_df.\n",
    "\n",
    "8. For each dataframe, produce the following columns:\n",
    "* title to hold the title\n",
    "* original to hold the original article/post content\n",
    "* clean to hold the normalized and tokenized original with the stopwords removed.\n",
    "* stemmed to hold the stemmed version of the cleaned data.\n",
    "* lemmatized to hold the lemmatized version of the cleaned data.\n",
    "\n",
    "9. Ask yourself:\n",
    "* If your corpus is 493KB, would you prefer to use stemmed or lemmatized text?\n",
    "* If your corpus is 25MB, would you prefer to use stemmed or lemmatized text?\n",
    "* If your corpus is 200TB of text and you're charged by the megabyte for your hosted computational resources, would you prefer to use stemmed or lemmatized text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e688ec4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "import json\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import pandas as pd\n",
    "import prepare\n",
    "import acquire\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08b39d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_clean(text):\n",
    "    \"\"\"\n",
    "    This function takes in a string and applies some basic text cleaning to it:\n",
    "    * Lowercase everything\n",
    "    * Normalize unicode characters\n",
    "    * Replace anything that is not a letter, number, whitespace or a single quote.\n",
    "    \"\"\"\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Normalize unicode characters\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    \n",
    "    # Replace everything that is not a letter, number, whitespace or a single quote with a space\n",
    "    text = re.sub(r\"[^a-z0-9\\s']\", ' ', text)\n",
    "    \n",
    "    # Remove extra whitespaces\n",
    "    text = re.sub(r\"\\s+\", ' ', text).strip()\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9f8e709",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    This function takes in a string and tokenizes all the words in the string.\n",
    "    \"\"\"\n",
    "    # Tokenize the text using the nltk library\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb6997c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(text):\n",
    "    \"\"\"\n",
    "    This function takes in a string and returns the text after applying stemming to all the words.\n",
    "    \"\"\"\n",
    "    # Tokenize the text using the nltk library\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Apply stemming to each token using the PorterStemmer from nltk\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "    \n",
    "    # Join the stemmed tokens back into a single string\n",
    "    stemmed_text = ' '.join(stemmed_tokens)\n",
    "    \n",
    "    return stemmed_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2903e6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(text):\n",
    "    \"\"\"\n",
    "    This function takes in a string and returns the text after applying lemmatization to each word.\n",
    "    \"\"\"\n",
    "    # Tokenize the text using the nltk library\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Apply lemmatization to each token using the WordNetLemmatizer from nltk\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    # Join the lemmatized tokens back into a single string\n",
    "    lemmatized_text = ' '.join(lemmatized_tokens)\n",
    "    \n",
    "    return lemmatized_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e6b8fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text, extra_words=None, exclude_words=None):\n",
    "    \"\"\"\n",
    "    This function takes in a string and returns the text after removing all the stopwords.\n",
    "    It has two optional parameters: extra_words and exclude_words to define additional stop words to include\n",
    "    and words that we don't want to remove.\n",
    "    \"\"\"\n",
    "    # Define the list of stopwords from the nltk library\n",
    "    stopword_list = stopwords.words('english')\n",
    "    \n",
    "    # Add any extra stop words to the list\n",
    "    if extra_words:\n",
    "        stopword_list.extend(extra_words)\n",
    "    \n",
    "    # Exclude any words from the stopword list\n",
    "    if exclude_words:\n",
    "        stopword_list = [word for word in stopword_list if word not in exclude_words]\n",
    "    \n",
    "    # Tokenize the text using the nltk library\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Remove the stop words from the tokens\n",
    "    filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
    "    \n",
    "    # Join the filtered tokens back into a single string\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    \n",
    "    return filtered_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6baa00c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df = acquire.get_news_articles_data(refresh=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "731b2470",
   "metadata": {},
   "outputs": [],
   "source": [
    "codeup_df = acquire.get_blog_articles_data(refresh=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f09127e8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title  \\\n",
      "0  whatsapp responds to int'l calls scam announce...   \n",
      "1  beyonce wears colour changing dress during con...   \n",
      "2  complaint filed against prabhas kriti sanon's ...   \n",
      "3   gauahar khan zaid darbar blessed with a baby boy   \n",
      "4  yuzvendra chahal creates history takes most wi...   \n",
      "\n",
      "                                             content  category  \\\n",
      "0  whatsapp has ramped up its ai and machine lear...  national   \n",
      "1  singer beyonce wore a colour changing dress du...  national   \n",
      "2  a complaint has been filed against prabhas and...  national   \n",
      "3  actress gauahar khan and her husband zaid darb...  national   \n",
      "4  rr leg spinner yuzvendra chahal has created hi...  national   \n",
      "\n",
      "                                            original  \\\n",
      "0  [whatsapp, has, ramped, up, its, ai, and, mach...   \n",
      "1  [singer, beyonce, wore, a, colour, changing, d...   \n",
      "2  [a, complaint, has, been, filed, against, prab...   \n",
      "3  [actress, gauahar, khan, and, her, husband, za...   \n",
      "4  [rr, leg, spinner, yuzvendra, chahal, has, cre...   \n",
      "\n",
      "                                             stemmed  \\\n",
      "0  whatsapp ha ramp up it ai and machin learn sys...   \n",
      "1  singer beyonc wore a colour chang dress dure h...   \n",
      "2  a complaint ha been file against prabha and kr...   \n",
      "3  actress gauahar khan and her husband zaid darb...   \n",
      "4  rr leg spinner yuzvendra chahal ha creat histo...   \n",
      "\n",
      "                                          lemmatized  \\\n",
      "0  whatsapp ha ramped up it ai and machine learni...   \n",
      "1  singer beyonce wore a colour changing dress du...   \n",
      "2  a complaint ha been filed against prabhas and ...   \n",
      "3  actress gauahar khan and her husband zaid darb...   \n",
      "4  rr leg spinner yuzvendra chahal ha created his...   \n",
      "\n",
      "                                               clean  \n",
      "0  whatsapp ramped ai machine learning systems de...  \n",
      "1  singer beyonce wore colour changing dress rena...  \n",
      "2  complaint filed prabhas kriti sanon 's film 'a...  \n",
      "3  actress gauahar khan husband zaid darbar share...  \n",
      "4  rr leg spinner yuzvendra chahal created histor...  \n"
     ]
    }
   ],
   "source": [
    "# apply basic text cleaning\n",
    "news_df['title'] = news_df['title'].apply(basic_clean)\n",
    "\n",
    "# apply basic text cleaning\n",
    "news_df['content'] = news_df['content'].apply(basic_clean)\n",
    "\n",
    "# tokenize the text\n",
    "news_df['original'] = news_df['content'].apply(tokenize)\n",
    "\n",
    "# apply stemming to the tokens\n",
    "news_df['stemmed'] = news_df['content'].apply(stem)\n",
    "\n",
    "# apply lemmatization to the tokens\n",
    "news_df['lemmatized'] = news_df['content'].apply(lemmatize)\n",
    "\n",
    "# remove stop words from the tokens\n",
    "news_df['clean'] = news_df['content'].apply(remove_stopwords)\n",
    "\n",
    "# print the first few rows of the data frame\n",
    "print(news_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ecee9307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title  \\\n",
      "0    women in tech panelist spotlight magdalena rahn   \n",
      "1  women in tech panelist spotlight rachel robbin...   \n",
      "2      women in tech panelist spotlight sarah mellor   \n",
      "3  women in tech panelist spotlight madeleine capper   \n",
      "4  black excellence in tech panelist spotlight wi...   \n",
      "\n",
      "                                             content  \\\n",
      "0  codeup is hosting a women in tech panel in hon...   \n",
      "1  codeup is hosting a women in tech panel in hon...   \n",
      "2  codeup is hosting a women in tech panel in hon...   \n",
      "3  codeup is hosting a women in tech panel in hon...   \n",
      "4  codeup is hosting a black excellence in tech p...   \n",
      "\n",
      "                                            original  \\\n",
      "0  [codeup, is, hosting, a, women, in, tech, pane...   \n",
      "1  [codeup, is, hosting, a, women, in, tech, pane...   \n",
      "2  [codeup, is, hosting, a, women, in, tech, pane...   \n",
      "3  [codeup, is, hosting, a, women, in, tech, pane...   \n",
      "4  [codeup, is, hosting, a, black, excellence, in...   \n",
      "\n",
      "                                             stemmed  \\\n",
      "0  codeup is host a women in tech panel in honor ...   \n",
      "1  codeup is host a women in tech panel in honor ...   \n",
      "2  codeup is host a women in tech panel in honor ...   \n",
      "3  codeup is host a women in tech panel in honor ...   \n",
      "4  codeup is host a black excel in tech panel in ...   \n",
      "\n",
      "                                          lemmatized  \\\n",
      "0  codeup is hosting a woman in tech panel in hon...   \n",
      "1  codeup is hosting a woman in tech panel in hon...   \n",
      "2  codeup is hosting a woman in tech panel in hon...   \n",
      "3  codeup is hosting a woman in tech panel in hon...   \n",
      "4  codeup is hosting a black excellence in tech p...   \n",
      "\n",
      "                                               clean  \n",
      "0  codeup hosting women tech panel honor womens h...  \n",
      "1  codeup hosting women tech panel honor womens h...  \n",
      "2  codeup hosting women tech panel honor womens h...  \n",
      "3  codeup hosting women tech panel honor womens h...  \n",
      "4  codeup hosting black excellence tech panel hon...  \n"
     ]
    }
   ],
   "source": [
    "# apply basic text cleaning\n",
    "codeup_df['title'] = codeup_df['title'].apply(basic_clean)\n",
    "\n",
    "# apply basic text cleaning\n",
    "codeup_df['content'] = codeup_df['content'].apply(basic_clean)\n",
    "\n",
    "# tokenize the text\n",
    "codeup_df['original'] = codeup_df['content'].apply(tokenize)\n",
    "\n",
    "# apply stemming to the tokens\n",
    "codeup_df['stemmed'] = codeup_df['content'].apply(stem)\n",
    "\n",
    "# apply lemmatization to the tokens\n",
    "codeup_df['lemmatized'] = codeup_df['content'].apply(lemmatize)\n",
    "\n",
    "# remove stop words from the tokens\n",
    "codeup_df['clean'] = codeup_df['content'].apply(remove_stopwords)\n",
    "\n",
    "# print the first few rows of the data frame\n",
    "print(codeup_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3766da",
   "metadata": {},
   "source": [
    "9. If your corpus is 493KB, would you prefer to use stemmed or lemmatized text?\n",
    " * If the goal is to have a more precise and accurate analysis of the corpus, it might be better to use lemmatized text. However, if the goal is to quickly process the text and extract general patterns, stemmed text might be more appropriate\n",
    "\n",
    "9. If your corpus is 25MB, would you prefer to use stemmed or lemmatized text?\n",
    "* I prefer lemmatizing for accuracy most time. \n",
    "\n",
    "9. If your corpus is 200TB of text and you're charged by the megabyte for your hosted computational resources, would you prefer to use stemmed or lemmatized text?\n",
    "* I'd sacrifice accuracy for cost in this case and go with stemmed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36150397",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
